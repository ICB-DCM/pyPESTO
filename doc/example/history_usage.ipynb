{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Optimizer Convergence and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "This notebook is dedicated to additional insights into the optimization process, be it convergence properties or comparisons of two different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**After this notebook you can...**\n",
    "\n",
    "- Use visualization methods to compare optimizers in terms of performance\n",
    "- check convergence of a multistart optimization\n",
    "- get insights into the convergence history\n",
    "- restart an optimization from a history file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypesto.optimize as optimize\n",
    "import pypesto.visualize as visualize\n",
    "import petab\n",
    "import pypesto.petab\n",
    "import logging\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# name of the model that will also be the name of the python module\n",
    "model_name = \"boehm_JProteomeRes2014\"\n",
    "\n",
    "np.random.seed(3142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of this notebook, we create two optimization results done with different optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# create problem\n",
    "petab_yaml = f\"./{model_name}/{model_name}.yaml\"\n",
    "\n",
    "petab_problem = petab.Problem.from_yaml(petab_yaml)\n",
    "importer = pypesto.petab.PetabImporter(petab_problem)\n",
    "problem = importer.create_problem(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "scipy_optimizer = optimize.ScipyOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary storagefile...\n",
    "f_scipy = tempfile.NamedTemporaryFile(suffix=\".hdf5\", delete=False)\n",
    "fn_scipy = f_scipy.name\n",
    "\n",
    "# ... and corresponding history option\n",
    "history_options_scipy = pypesto.HistoryOptions(\n",
    "    trace_record=True, storage_file=fn_scipy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to the above:**\n",
    "\n",
    "In practice you should not use a temporary file, as it is removed after the run, while still creating overhead. There are two options you might choose from instead:\n",
    "\n",
    "- If you do not plan to save the optimization result, you can use a `MemoryHistory` by removing the `storage_file`-argument. This creates no overhead but is more demanding on the memory.\n",
    "- If you want to save your results (**recommended**) for any form of reusability, you can remove `f_$optimizer` and replace the `fn_$optimizer`assignment with `fn_$optimizer = \"filename_of_choice.hdf5\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_starts = 10\n",
    "\n",
    "# run optimization\n",
    "result_scipy = optimize.minimize(\n",
    "    problem=problem,\n",
    "    optimizer=scipy_optimizer,\n",
    "    n_starts=n_starts,\n",
    "    history_options=history_options_scipy,\n",
    "    filename=fn_scipy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step we compare the optimizers in terms of final objective function and robustness through a waterfall plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to check convergence of a single result. For this a summary and general visualizations such as waterfall-plots can be helpfull, but also specific optimizer_convergence visualization as well as history tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result_scipy.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waterfall plot\n",
    "visualize.waterfall(result_scipy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The waterfall plot is an overview of the final objective function values. They are ordered from best to worst. Similar colors indicate similar function values and potential local optima/mannifolds. In the best case scenario all values are assigned to a plateau indicating local optima and the best value is found more then once. Additionally we might want to check whether the gradients converged as well and whether we can find a pattern in specific reasons to stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.optimizer_convergence(result_scipy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually want the gradients to be very low, in order to actually ensure we are in a local optimum. If the results do not seem entirely promising, we might want to switch optimizers altogether, as different optimizers sometimes perform better for other problems. Additionally one can try changing the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to fides optimizer\n",
    "fides_optimizer = optimize.FidesOptimizer(\n",
    "    verbose=logging.WARN\n",
    ")\n",
    "f_fides = tempfile.NamedTemporaryFile(suffix=\".hdf5\", delete=False)\n",
    "fn_fides = f_fides.name\n",
    "history_options_fides = pypesto.HistoryOptions(\n",
    "    trace_record=True, storage_file=fn_fides\n",
    ")\n",
    "result_fides = optimize.minimize(\n",
    "    problem=problem,\n",
    "    optimizer=fides_optimizer,\n",
    "    n_starts=n_starts,\n",
    "    history_options=history_options_fides,\n",
    "    filename=fn_fides,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.waterfall([result_fides, result_scipy], legends=[\"Fides Optimizer\", \"Scipy Optimizer\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare various metrics of the results, such as time and number of evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.optimization_run_properties_per_multistart(\n",
    "        [result_fides, result_scipy], properties_to_plot=['time', 'n_fval'],\n",
    "        legends=[\"Fides Optimizer\", \"Scipy Optimizer\"] \n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to check how close the estimated guesses are together, for this we can employ the parameter visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.parameters(result_fides);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how the optimization trajectory looks like during the different runs, getting other reasons such as very flat landscapes, that can be additonal reasons for the optimization to stop. For this the we use the history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.optimizer_history(result_fides, trace_y=\"fval\")\n",
    "visualize.optimizer_history(result_fides, trace_y=\"gradnorm\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the function values are not monotonic. This is due to the optimization tracing line search evaluations as well. This allows us to investigate potential problems. Recurring patterns in the gradient norm together with miniscule to no changes in the function values indicate the optimizer to not be able to really find another next point or taking spiraling steps. In both cases the actual optimum is very hard to pinpoint.\n",
    "Lowering tolerances, increasing startpoints (up to a certain point), switching optimizers are all valid strategies in trying to overcome such issues. However, there is no recipe for all models and thus it is always important to investigate the optimization in terms of convergence, termination reasons and function evaluations, to get ideas on what to do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reloading from History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially when running large models on clusters, the optimization sometimes my stop due to unfortunate reasons (e.g. timeouts). In these cases, the history serves yet another purpose: retrieving finished and unfinished optimizations. Sometimes out of 100 starts, 80 might have already been terminated, in this case, investigating those 80 might already yield good results. In other cases, we might want to continue optimization from where we left of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result from history\n",
    "result_from_history = optimize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
