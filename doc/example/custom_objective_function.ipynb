{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Objective Function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "pyPESTO can not only do parameter estimation for PEtab and AMICI models, but is able to do so on any provided function.\n",
    "This is done by providing the objective with the function as well as possibly gradient and hessian.\n",
    "In this notebook, we will show a few different ways on how to do this. As sometimes manually providing the gradient and hessian is tedious, we will try to emphasize on the importance of those two."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After this notebook, you should ...\n",
    "* ... be able to create an objective from a given function.\n",
    "* ... be able to potentially add a gradient and hessian to the objective.\n",
    "* ... be able to run parameter estimation on the objective.\n",
    "* ... know the importance of gradient and hessian in terms of optimization speed and accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# install if not done yet\n",
    "# %pip install pypesto --quiet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import pypesto\n",
    "import pypesto.optimize as optimize\n",
    "import pypesto.profile as profile\n",
    "import pypesto.visualize as visualize\n",
    "\n",
    "# set a random seed\n",
    "np.random.seed(1912)\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Objective + Problem Definition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following we will use the [Rosenbrock Banana]() function, which we can directly get from scipy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first creation of the objective function is rather straightforward:\n",
    "We create it by providing a function, as well as gradient and hessian."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first type of objective defined through callables\n",
    "objective1 = pypesto.Objective(\n",
    "    fun=sp.optimize.rosen,\n",
    "    grad=sp.optimize.rosen_der,\n",
    "    hess=sp.optimize.rosen_hess,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second option is to provide a function that returns objective value, gradient and hessian (last two optional) all as a tuple.\n",
    "In this case, we just need to notify the pyPESTO objective of this through boolean values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# second type of objective\n",
    "def rosen2(x):\n",
    "    return (\n",
    "        sp.optimize.rosen(x),\n",
    "        sp.optimize.rosen_der(x),\n",
    "        sp.optimize.rosen_hess(x),\n",
    "    )\n",
    "\n",
    "\n",
    "objective2 = pypesto.Objective(fun=rosen2, grad=True, hess=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For later comparisons, we create two other objectives. One that is only provided with function and gradient, and one that only has the functional value, forcing it to rely on finite differences in optimization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# no hessian objective\n",
    "objective3 = pypesto.Objective(\n",
    "    fun=sp.optimize.rosen,\n",
    "    grad=sp.optimize.rosen_der,\n",
    ")\n",
    "\n",
    "# neither hessian nor gradient objective\n",
    "objective4 = pypesto.Objective(\n",
    "    fun=sp.optimize.rosen,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get from objective to a usable parameter estimation problem, we need to additionally provide the bounds of our parameters. We can customize it further, for this we refer to [the code](../../pypesto/problem/base.py)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dim_full = 15\n",
    "lb = -5 * np.ones((dim_full, 1))\n",
    "ub = 5 * np.ones((dim_full, 1))\n",
    "\n",
    "# for the sake of comparison, we create 100 starts within these bounds\n",
    "x_guesses = np.random.uniform(-5, 5, (100, dim_full))\n",
    "\n",
    "problem1 = pypesto.Problem(\n",
    "    objective=objective1, lb=lb, ub=ub, x_guesses=x_guesses\n",
    ")\n",
    "problem2 = pypesto.Problem(\n",
    "    objective=objective2, lb=lb, ub=ub, x_guesses=x_guesses\n",
    ")\n",
    "problem3 = pypesto.Problem(\n",
    "    objective=objective3, lb=lb, ub=ub, x_guesses=x_guesses\n",
    ")\n",
    "problem4 = pypesto.Problem(\n",
    "    objective=objective4, lb=lb, ub=ub, x_guesses=x_guesses\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Optimization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optimize.ScipyOptimizer()\n",
    "# engine\n",
    "engine = pypesto.engine.MultiProcessEngine()\n",
    "# starts\n",
    "n_starts = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a first comparison, we time each optimization. We also use the same optimizer and engine for all optimizations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run optimization of problem 1\n",
    "result1 = optimize.minimize(\n",
    "    problem=problem1, optimizer=optimizer, n_starts=n_starts, engine=engine\n",
    ")\n",
    "# run optimization of problem 2\n",
    "result2 = optimize.minimize(\n",
    "    problem=problem2, optimizer=optimizer, n_starts=n_starts, engine=engine\n",
    ")\n",
    "# run optimization of problem 3\n",
    "result3 = optimize.minimize(\n",
    "    problem=problem3, optimizer=optimizer, n_starts=n_starts, engine=engine\n",
    ")\n",
    "# run optimization of problem 4\n",
    "result4 = optimize.minimize(\n",
    "    problem=problem4, optimizer=optimizer, n_starts=n_starts, engine=engine\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a first step, let us take a look at the different result summaries:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(\n",
    "    Markdown(\"# Result 1\\n\" + result1.optimize_result.summary(disp_best=False))\n",
    ")\n",
    "display(\n",
    "    Markdown(\"# Result 2\\n\" + result2.optimize_result.summary(disp_best=False))\n",
    ")\n",
    "display(\n",
    "    Markdown(\"# Result 3\\n\" + result3.optimize_result.summary(disp_best=False))\n",
    ")\n",
    "display(\n",
    "    Markdown(\"# Result 4\\n\" + result4.optimize_result.summary(disp_best=False))\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see already a big difference between the first three and the fourth. The one without gradients took approximately five times as long to finish the optimization as the other three. The best value found ist also not the same as for the others. But the biggest difference is probably the fact that while the first three all converged in all their 100 starts, the one without gradient reach the maximum number of evaluations in most to all cases. Keep in mind: **All 100 starts were the same for all problems**.\n",
    "\n",
    "A small detail here:\n",
    "When comparing the first three results, one may notice two things. For one, leaving out the hessian seems not make any difference. And additionally, while they are rather close in speed compared to the fourth one, the second one sticks out to be somewhat slower.\n",
    "This is mainly due to the fact that we chose the scipy optimizer \"l-bfgs-b\", which does not support the usage of a hessian, for the sake of comparing all four of them. This also explains why the second one is slower, as (by construction), whether needed or not, problem2 will evaluate the hessian."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# waterfalls\n",
    "visualize.waterfall(result1)\n",
    "visualize.waterfall(result2)\n",
    "visualize.waterfall(result3)\n",
    "visualize.waterfall(result4);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see here already the stark difference between the problem definitions. In order to compare things easier, we can also plot all results in one waterfall plot:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot one list of waterfalls\n",
    "visualize.waterfall(\n",
    "    [result1, result2, result3, result4],\n",
    "    legends=[\"Problem 1\", \"Problem 2\", \"Problem 3\", \"Problem 4\"],\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also take a look at the parameters, that each optimizer found:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot parameters\n",
    "ax = visualize.parameters(\n",
    "    result1,\n",
    ")\n",
    "ax.set_title(\"Estimated parameters problem 1\")\n",
    "ax2 = visualize.parameters(\n",
    "    [result4, result1, result2, result3],\n",
    "    legends=[\"Problem 4\", \"Problem 1\", \"Problem 2\", \"Problem 3\"],\n",
    ")\n",
    "ax2.set_title(\"Estimated parameters all problems\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Profiling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to create profiles for our parameters to know more about their uncertainties. We shall create profiles for problem 1 and 4. For problem 1 specifically, we create two profiles, as we have seen, that there are two distinct optima, so we want to start our profile likelihood from both optima."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we create references for each \"best point\":\n",
    "ref = {\n",
    "    \"x\": result1.optimize_result.x[0],\n",
    "    \"fval\": result1.optimize_result.fval[0],\n",
    "    \"color\": [0.2, 0.4, 1.0, 1.0],\n",
    "    \"legend\": \"First optimum problem 1\",\n",
    "}\n",
    "ref = visualize.create_references(ref)[0]\n",
    "# we create references for each \"best point\":\n",
    "ref2 = {\n",
    "    \"x\": result1.optimize_result.x[90],\n",
    "    \"fval\": result1.optimize_result.fval[90],\n",
    "    \"color\": [0.4, 1.0, 0.2, 1.0],\n",
    "    \"legend\": \"Second optimum problem 1\",\n",
    "}\n",
    "ref2 = visualize.create_references(ref2)[0]\n",
    "# we create references for each \"best point\":\n",
    "ref4 = {\n",
    "    \"x\": result4.optimize_result.x[0],\n",
    "    \"fval\": result4.optimize_result.fval[0],\n",
    "    \"color\": [0.2, 0.4, 1.0, 1.0],\n",
    "    \"legend\": \"First optimum problem 4\",\n",
    "}\n",
    "ref4 = visualize.create_references(ref4)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute profiles\n",
    "profile_options = profile.ProfileOptions(whole_path=True)\n",
    "\n",
    "result1 = profile.parameter_profile(\n",
    "    problem=problem1,\n",
    "    result=result1,\n",
    "    optimizer=optimizer,\n",
    "    profile_index=np.array([0, 1, 3, 5]),\n",
    "    result_index=0,\n",
    "    profile_options=profile_options,\n",
    "    filename=None,\n",
    ")\n",
    "\n",
    "# compute profiles from second optimum\n",
    "result1 = profile.parameter_profile(\n",
    "    problem=problem1,\n",
    "    result=result1,\n",
    "    optimizer=optimizer,\n",
    "    profile_index=np.array([0, 1, 3, 5]),\n",
    "    result_index=90,\n",
    "    profile_options=profile_options,\n",
    "    filename=None,\n",
    ")\n",
    "result4 = profile.parameter_profile(\n",
    "    problem=problem4,\n",
    "    result=result4,\n",
    "    optimizer=optimizer,\n",
    "    profile_index=np.array([0, 1, 3, 5]),\n",
    "    result_index=0,\n",
    "    profile_options=profile_options,\n",
    "    filename=None,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# specify the parameters, for which profiles should be computed\n",
    "ax = visualize.profiles(\n",
    "    result1,\n",
    "    profile_indices=[0, 1, 3, 5],\n",
    "    reference=[ref, ref2],\n",
    "    profile_list_ids=[0, 1],\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = visualize.profiles(\n",
    "    result4,\n",
    "    profile_indices=[0, 1, 3, 5],\n",
    "    reference=[ref4],\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see here, the second optimum disagrees only in the very first value. Here it becomes apparent that it is only a local optimum, as the profile for the second optimum is very flat. Comparing the profiles of proplem 1 and 4, we can see, that while the convergence of problem4 was quite bad, the profiles look really similar."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When computing the profiles is computationally too demanding, it is possible to employ to at least consider a normal approximation with covariance matrix given by the Hessian or FIM at the optimal parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result1 = profile.approximate_parameter_profile(\n",
    "    problem=problem1,\n",
    "    result=result1,\n",
    "    profile_index=np.array([0, 1, 3, 5]),\n",
    "    result_index=0,\n",
    "    n_steps=1000,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These approximate profiles require at most one additional function evaluation, can however yield substantial approximation errors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "axes = visualize.profiles(\n",
    "    result1,\n",
    "    profile_indices=[0, 1, 3, 5],\n",
    "    profile_list_ids=[0, 2],\n",
    "    ratio_min=0.01,\n",
    "    colors=[(1, 0, 0, 1), (0, 0, 1, 1)],\n",
    "    legends=[\n",
    "        \"Optimization-based profile\",\n",
    "        \"Local profile approximation\",\n",
    "    ],\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize.profile_cis(\n",
    "    result1, confidence_level=0.95, profile_list=2, show_bounds=True\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
